{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r\".\\new_generated.xlsx\",)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_5 = pd.read_excel(r\"generated_gpt3.5_1.xlsx\")\n",
    "data[\"generated_gpt3.5\"] = gpt3_5[\"generated_gpt-3.5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "original_texts = data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6997\n",
      "Recall: 0.7182\n",
      "F1-score: 0.7085\n",
      "Rouge1: 0.0467\n",
      "Rouge2: 0.0000\n",
      "RougeL: 0.0467\n",
      "BLEU: 0.1954\n"
     ]
    }
   ],
   "source": [
    "# LLM VS GPT3.5\n",
    "\n",
    "hypotheses = data['llm_deletions'].tolist()\n",
    "references = data['generated_gpt3.5'].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6024\n",
      "Recall: 0.6535\n",
      "F1-score: 0.6260\n",
      "Rouge1: 0.0467\n",
      "Rouge2: 0.0000\n",
      "RougeL: 0.0467\n",
      "BLEU: 0.1954\n"
     ]
    }
   ],
   "source": [
    "# LLM VS Target\n",
    "\n",
    "hypotheses = data['llm_deletions'].tolist()\n",
    "references = data['target'].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6726\n",
      "Recall: 0.6781\n",
      "F1-score: 0.6750\n",
      "Rouge1: 0.0226\n",
      "Rouge2: 0.0073\n",
      "RougeL: 0.0226\n",
      "BLEU: 0.0747\n"
     ]
    }
   ],
   "source": [
    "# Logic VS GPT-3.5\n",
    "\n",
    "hypotheses = data['logic_deletions'].tolist()\n",
    "references = data['generated_gpt3.5'].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5850\n",
      "Recall: 0.6313\n",
      "F1-score: 0.6062\n",
      "Rouge1: 0.0226\n",
      "Rouge2: 0.0073\n",
      "RougeL: 0.0226\n",
      "BLEU: 0.0747\n"
     ]
    }
   ],
   "source": [
    "# Logic VS Target\n",
    "\n",
    "hypotheses = data['logic_deletions'].tolist()\n",
    "references = data['target'].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6108\n",
      "Recall: 0.6629\n",
      "F1-score: 0.6348\n",
      "Rouge1: 0.0362\n",
      "Rouge2: 0.0000\n",
      "RougeL: 0.0362\n",
      "BLEU: 0.2731\n"
     ]
    }
   ],
   "source": [
    "# LLM VS Target\n",
    "\n",
    "hypotheses = data['llm_deletions4o'].tolist()\n",
    "references = data['target'][:len(hypotheses)].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7119\n",
      "Recall: 0.7366\n",
      "F1-score: 0.7237\n",
      "Rouge1: 0.0362\n",
      "Rouge2: 0.0000\n",
      "RougeL: 0.0362\n",
      "BLEU: 0.2731\n"
     ]
    }
   ],
   "source": [
    "# LLM VS GPT-3.5\n",
    "\n",
    "hypotheses = data['llm_deletions4o'].tolist()\n",
    "references = data['generated_gpt3.5'][:len(hypotheses)].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Precision: 0.6072\n",
      "BERT Recall: 0.6511\n",
      "BERT F1-score: 0.6273\n",
      "Rouge1: 0.0379\n",
      "Rouge2: 0.0033\n",
      "RougeL: 0.0379\n",
      "BLEU: 0.0591\n"
     ]
    }
   ],
   "source": [
    "# LLM VS Target\n",
    "\n",
    "hypotheses = data['logic_deletions4o'].tolist()\n",
    "references = data['target'][:len(hypotheses)].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Precision: 0.6072\n",
      "BERT Recall: 0.6511\n",
      "BERT F1-score: 0.6273\n",
      "Rouge1: 0.0379\n",
      "Rouge2: 0.0033\n",
      "RougeL: 0.0379\n",
      "BLEU: 0.0591\n"
     ]
    }
   ],
   "source": [
    "# Logic VS Target\n",
    "\n",
    "hypotheses = data['logic_deletions4o'].tolist()\n",
    "references = data['target'][:len(hypotheses)].tolist()\n",
    "P, R, F1 = [], [], []\n",
    "\n",
    "for hypothesis, reference in zip(hypotheses, references):\n",
    "    P_, R_, F1_ = score([hypothesis], [reference], lang=\"hi\")\n",
    "    P.append(P_.item())\n",
    "    R.append(R_.item())\n",
    "    F1.append(F1_.item())\n",
    "    \n",
    "averaged_precision = sum(P) / len(P)\n",
    "avergaged_recall = sum(R) / len(R)\n",
    "averaged_f1 = sum(F1) / len(F1)\n",
    "print(f\"BERT Precision: {averaged_precision:.4f}\")\n",
    "print(f\"BERT Recall: {avergaged_recall:.4f}\")\n",
    "print(f\"BERT F1-score: {averaged_f1:.4f}\")\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "for i in range(len(hypotheses)):\n",
    "    scores = scorer.score(original_texts[i], hypotheses[i])\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "averaged_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "averaged_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "averaged_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "print(f\"Rouge1: {averaged_rouge1:.4f}\")\n",
    "print(f\"Rouge2: {averaged_rouge2:.4f}\")\n",
    "print(f\"RougeL: {averaged_rougeL:.4f}\")\n",
    "\n",
    "bleu_scores = []\n",
    "original_texts_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in original_texts]\n",
    "hypotheses_tokenized = [\" \".join(indic_tokenize.trivial_tokenize(text, lang=\"hi\")) for text in hypotheses]\n",
    "for i in range(len(hypotheses)):\n",
    "    bleu = sacrebleu.corpus_bleu([hypotheses_tokenized[i]], [[original_texts_tokenized[i]]])\n",
    "    bleu_scores.append(bleu.score)\n",
    "averaged_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU: {averaged_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
