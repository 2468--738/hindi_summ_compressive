{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "from openai import OpenAI\n",
    "API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = API_KEY\n",
    "openai_api_base = \"server url\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/mixtral-8x22B-instruct-v0.1\"\n",
    "stopsequence = [\"?\", \".\", \"<EOD>\", \"[/User]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    "    default_headers={\"RITS_API_KEY\": openai_api_key},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rits_request(queries):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": queries}],\n",
    "        max_tokens=4000,\n",
    "        stop=stopsequence,\n",
    "        temperature=0.75,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    result = completion.choices[0].message.content.strip()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logics for the various phrases\n",
    "\n",
    "def remove_adjectival_phrases(text: list):\n",
    "    def remove_from_sent(sentence: list):\n",
    "        indices_to_be_removed = []\n",
    "        phrase = []\n",
    "        \n",
    "        for i, entry in enumerate(sentence):\n",
    "            word = entry[\"word\"]\n",
    "            label = entry[\"label\"]\n",
    "            \n",
    "            if label in [\"RB\", \"DEM\", \"JJ\", \"INTF\"]:\n",
    "                phrase.append(i)\n",
    "            else:\n",
    "                if phrase:\n",
    "                    indices_to_be_removed.extend(phrase)\n",
    "                    phrase = [] \n",
    "                    \n",
    "        new_sent = []\n",
    "        for i in range(len(sentence)):\n",
    "            if indices_to_be_removed and i == indices_to_be_removed[0]:\n",
    "                del indices_to_be_removed[0]\n",
    "            else:\n",
    "                new_sent.append(sentence[i])\n",
    "        return new_sent\n",
    "                \n",
    "    return [remove_from_sent(sentence) for sentence in text]\n",
    "\n",
    "\n",
    "def remove_adveribial_phrases(text):\n",
    "    def remove_from_sent(sentence: list):\n",
    "        indices_to_be_removed = []\n",
    "        phrase = []\n",
    "        \n",
    "        for i, entry in enumerate(sentence):\n",
    "            word = entry[\"word\"]\n",
    "            label = entry[\"label\"]\n",
    "            \n",
    "            if label in ['RB', 'INTF', 'RP', 'QF', 'QCC', 'RBC']:\n",
    "                phrase.append(i)\n",
    "            \n",
    "            elif label == 'JJ' and phrase and sentence[phrase[-1]]['label'] in ['INTF', 'RB']:\n",
    "                phrase.append(i)\n",
    "            \n",
    "            elif phrase:\n",
    "                indices_to_be_removed.extend(phrase)\n",
    "                phrase = [] \n",
    "                  \n",
    "        new_sent = []\n",
    "        for i in range(len(sentence)):\n",
    "            if indices_to_be_removed and i == indices_to_be_removed[0]:\n",
    "                del indices_to_be_removed[0]\n",
    "            else:\n",
    "                new_sent.append(sentence[i])\n",
    "        return new_sent\n",
    "                \n",
    "    return [remove_from_sent(sentence) for sentence in text]\n",
    "\n",
    "\n",
    "def remove_fragments(text):\n",
    "    def is_not_fragment(sentence):\n",
    "        has_noun = False\n",
    "        has_verb = False\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word['label'] in ['PRP', 'NNP', 'NN', 'NNPC', 'NNC', 'DEM', 'PRPC']:\n",
    "                has_noun = True\n",
    "                \n",
    "            if word['label'] in ['VM', 'VAUX', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "                has_verb = True\n",
    "                \n",
    "        return has_noun and has_verb\n",
    "    \n",
    "    return [sentence for sentence in text if is_not_fragment(sentence)]   \n",
    "\n",
    "\n",
    "def remove_parentheticals(text: list):\n",
    "    def remove_from_sent(sentence: list):\n",
    "        is_parenthetical = False\n",
    "        \n",
    "        for i, entry in enumerate(sentence):\n",
    "            word = entry['word']\n",
    "            if word == '(':\n",
    "                is_parenthetical = True\n",
    "                \n",
    "            if word == ')':\n",
    "                is_parenthetical = False\n",
    "                sentence[i] = \"deleted\"\n",
    "                \n",
    "            if is_parenthetical:\n",
    "                sentence[i] = \"deleted\"\n",
    "        print(sentence)\n",
    "        return [word for word in sentence if word != \"deleted\"]\n",
    "        \n",
    "    return [remove_from_sent(sentence) for sentence in text]\n",
    "\n",
    "\n",
    "def remove_prepositional_phrases(text):\n",
    "    def remove_from_sent(sentence: list):\n",
    "        indices_to_be_removed = []\n",
    "        phrase = []\n",
    "        \n",
    "        for i, entry in enumerate(sentence):\n",
    "            word = entry[\"word\"]\n",
    "            label = entry[\"label\"]\n",
    "            \n",
    "            if label in ['NN', 'NNP', 'NNC', 'NNPC', 'NNS']:\n",
    "                phrase.append(i)\n",
    "            elif label == 'PSP':\n",
    "                if phrase:\n",
    "                    phrase.append(i)\n",
    "                    indices_to_be_removed.extend(phrase)\n",
    "                    phrase = []\n",
    "            else:\n",
    "                if phrase:\n",
    "                    phrase = []\n",
    "                    \n",
    "        new_sent = []\n",
    "        for i in range(len(sentence)):\n",
    "            if indices_to_be_removed and i == indices_to_be_removed[0]:\n",
    "                del indices_to_be_removed[0]\n",
    "            else:\n",
    "                new_sent.append(sentence[i])\n",
    "                \n",
    "        return new_sent\n",
    "                \n",
    "    return [remove_from_sent(sentence) for sentence in text]\n",
    "\n",
    "\n",
    "def delete_required_parts(text: list):\n",
    "    new_text = remove_adveribial_phrases(text)\n",
    "    new_text = remove_adjectival_phrases(new_text)\n",
    "    new_text = remove_fragments(new_text)\n",
    "    new_text = remove_parentheticals(new_text)\n",
    "    new_text = remove_prepositional_phrases(new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def get_text_output(text: list):\n",
    "    final_text = []\n",
    "    for sentence in text:\n",
    "        final_text.append(' '.join([word['word'] for word in sentence]))\n",
    "    return ' '.join(final_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('hi')\n",
    "nlp = stanza.Pipeline('hi')\n",
    "\n",
    "def get_pos_tags(text, pipeline):\n",
    "    text_tagged = []\n",
    "    \n",
    "    doc = pipeline(text)\n",
    "    for sentence in doc.sentences:\n",
    "        sent = []\n",
    "        for word in sentence.words:\n",
    "            sent.append({\"word\": word.text, \"label\": word.xpos})\n",
    "        text_tagged.append(sent) \n",
    "        \n",
    "    return text_tagged   \n",
    "    \n",
    "# Example output\n",
    "hindi_text = \"अफ्रीका का वन्यजीवन दुनिया भर में अपनी विविधता और अनोखेपन के लिए प्रसिद्ध है। यहाँ के विस्तृत सवाना, घने जंगल और विशाल रेगिस्तान असंख्य प्रजातियों का घर हैं। अफ्रीका में शेर, चीता, तेंदुआ, हाथी, गैंडा और भैंस जैसे बड़े जानवरों से लेकर जिराफ, ज़ेब्रा और हिप्पोपोटामस तक कई अद्भुत प्राणी पाए जाते हैं। इसके अलावा, यहाँ पक्षियों की भी हजारों प्रजातियाँ देखने को मिलती हैं। अफ्रीका के राष्ट्रीय उद्यान और संरक्षित क्षेत्र, जैसे कि मसाई मारा, क्रूगर नेशनल पार्क और सेरेनगेटी, वन्यजीवन संरक्षण के महत्वपूर्ण केंद्र हैं। हालाँकि, शिकार और वनों की कटाई के कारण कई प्रजातियाँ खतरे में हैं। इनके संरक्षण के लिए अनेक प्रयास किए जा रहे हैं, ताकि भविष्य की पीढ़ियाँ भी इस अद्भुत वन्यजीवन का आनंद ले सकें।\"\n",
    "tagged_hindi_text = get_pos_tags(hindi_text, pipeline=nlp)\n",
    "for sentence in tagged_hindi_text:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"Generate a summary of the following piece of text in Hindi Language in %d words. Do not use any words fron any other language. Be concise and informative. Do not generate any text other than the summary. The text starts from below:\\n%s\"\n",
    "compression_prompt = \"I want to compress the below Hindi text by removing parts that are not required to get the complete meaning of the text. Please delete the words and phrases that can be deleted without affecting the meaning of the text. Do not generate words other than the compressed summary. The summary starts from below:\\n%s\"\n",
    "add_more_words_prompt = \"I have a piece of Hindi text below:\\n%s\\nCould you please add %d more words to the following summary so that it becomes more informative and complete? Do not use any words other than Hindi language. Generate only the summary, and no other words. The summary starts from below:\\n%s\"\n",
    "system_role = \"You are an expert Hindi linguist.\"\n",
    "\n",
    "\n",
    "def generate_summary_pipeline(text, summary_prompt, compression_prompt, add_more_words_prompt, compression_factor):\n",
    "    words_in_text = len(text.split())\n",
    "    words_in_final_summary = int(words_in_text * compression_factor)\n",
    "    global system_role\n",
    "    \n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    \n",
    "    # generate summary\n",
    "    messages=[\n",
    "                {\"role\": \"system\", \"content\": system_role},\n",
    "                {\"role\": \"user\", \"content\": summary_prompt % (words_in_final_summary, text)},\n",
    "            ]\n",
    "    first_summary = rits_request(messages)\n",
    "    \n",
    "    # compress the summary\n",
    "    messages=[\n",
    "                {\"role\": \"system\", \"content\": system_role},\n",
    "                {\"role\": \"user\", \"content\": compression_prompt % first_summary},\n",
    "            ]\n",
    "    first_summary = rits_request(messages)\n",
    "    \n",
    "    print(\"Loop 0:\", first_summary)\n",
    "\n",
    "    current_summary_words = len(first_summary.split())\n",
    "    new_summary = first_summary\n",
    "    \n",
    "    loops = 0\n",
    "    try:\n",
    "        while abs(words_in_final_summary - current_summary_words) > 20 and loops < 10:\n",
    "            print(\"Required words:\", words_in_final_summary)\n",
    "            print(\"Current words:\", current_summary_words)\n",
    "            \n",
    "            \n",
    "            # add words to compressed summary\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_role},\n",
    "                    {\"role\": \"user\", \"content\": add_more_words_prompt % (text, words_in_final_summary - current_summary_words, new_summary)},\n",
    "                ]\n",
    "            new_summary = rits_request(messages)\n",
    "            \n",
    "            # compress the summary\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_role},\n",
    "                    {\"role\": \"user\", \"content\": compression_prompt % new_summary},\n",
    "                ]\n",
    "            new_summary = rits_request(messages)\n",
    "            \n",
    "            current_summary_words = len(new_summary.split())\n",
    "            loops += 1\n",
    "            \n",
    "            print(f\"Loop {loops}:\", new_summary)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return (loops, new_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_provided = 50\n",
    "\n",
    "data = pandas.read_excel(r'./new_generated.xlsx', nrows=rows_provided)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    summaries = data[f\"llm_deletions_{model_name}\"].tolist()\n",
    "except:\n",
    "    summaries = [\"\"] * 50\n",
    "    \n",
    "for i, text in enumerate(data['text']):\n",
    "    if summaries[i] != \"\":\n",
    "        continue\n",
    "    print(\"Text:\", text)\n",
    "    print()\n",
    "    summary = generate_summary_pipeline(text, summary_prompt, compression_prompt, add_more_words_prompt, 0.2)\n",
    "    summaries[i] = summary[1]\n",
    "    print(summaries)\n",
    "    print(\"Summary:\", summary)\n",
    "    data[f'llm_deletions_{model_name}'] = summaries\n",
    "    data.to_excel(r'.\\new_generated.xlsx', index=False)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "data[f'llm_deletions_{model_name}'] = summaries\n",
    "data.to_excel(r'.\\new_generated.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = \"Generate a summary of the following piece of text in Hindi Language in %d words. Do not use any words fron any other language. Be concise and informative. Do not generate any text other than the summary. The text starts from below:\\n%s\"\n",
    "compression_prompt = \"I want to compress the below Hindi text by removing parts that are not required to get the complete meaning of the text. Please delete the words and phrases that can be deleted without affecting the meaning of the text. Do not generate words other than the compressed summary. The summary starts from below:\\n%s\"\n",
    "add_more_words_prompt = \"I have a piece of Hindi text below:\\n%s\\nCould you please add %d more words to the following summary so that it becomes more informative and complete? Do not use any words other than Hindi language. Generate only the summary, and no other words. The summary starts from below:\\n%s\"\n",
    "system_role = \"You are an expert Hindi linguist.\"\n",
    "\n",
    "def generate_summary_pipeline_logic(text, summary_prompt, add_more_words_prompt, compression_factor):\n",
    "    words_in_text = len(text.split())\n",
    "    words_in_final_summary = int(words_in_text * compression_factor)\n",
    "    global system_role\n",
    "    \n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "    \n",
    "    # generate summary\n",
    "    messages=[\n",
    "                {\"role\": \"system\", \"content\": system_role},\n",
    "                {\"role\": \"user\", \"content\": summary_prompt % (words_in_final_summary, text)},\n",
    "            ]\n",
    "    first_summary = rits_request(messages)\n",
    "    \n",
    "    # compress the summary\n",
    "    processed_text = get_pos_tags(first_summary, pipeline=nlp)\n",
    "    deleted_required_parts = delete_required_parts(processed_text)\n",
    "    first_summary = get_text_output(deleted_required_parts)\n",
    "    \n",
    "    print(\"Loop 0:\", first_summary)\n",
    "\n",
    "    current_summary_words = len(first_summary.split())\n",
    "    new_summary = first_summary\n",
    "    \n",
    "    loops = 0\n",
    "    try:\n",
    "        while words_in_final_summary - current_summary_words > 20 and loops < 10:\n",
    "            print(\"Required words:\", words_in_final_summary)\n",
    "            print(\"Current words:\", current_summary_words)\n",
    "            \n",
    "            # add words to compressed summary\n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_role},\n",
    "                    {\"role\": \"user\", \"content\": add_more_words_prompt % (text, words_in_final_summary - current_summary_words, new_summary)},\n",
    "                ]\n",
    "            new_summary = rits_request(messages)\n",
    "            \n",
    "            # compress the summary\n",
    "            processed_text = get_pos_tags(new_summary, pipeline=nlp)\n",
    "            deleted_required_parts = delete_required_parts(processed_text)\n",
    "            new_summary = get_text_output(deleted_required_parts)\n",
    "            \n",
    "            current_summary_words = len(new_summary.split())\n",
    "            loops += 1\n",
    "            \n",
    "            print(f\"Loop {loops}:\", new_summary)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return (loops, new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    summaries = data[f\"logic_deletions_{model_name}\"].tolist()\n",
    "except:\n",
    "    summaries = [\"\"] * 50\n",
    "    \n",
    "for i, text in enumerate(data['text']):\n",
    "    if summaries[i] != \"\":\n",
    "        continue\n",
    "    print(\"Text:\", text)\n",
    "    print()\n",
    "    summary = generate_summary_pipeline(text, summary_prompt, compression_prompt, add_more_words_prompt, 0.2)\n",
    "    summaries[i] = summary[1]\n",
    "    print(summaries)\n",
    "    print(\"Summary:\", summary)\n",
    "    data[f'logic_deletions_{model_name}'] = summaries\n",
    "    data.to_excel(r'.\\new_generated.xlsx', index=False)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "data[f'logic_deletions{model_name}'] = summaries\n",
    "data.to_excel(r'.\\new_generated.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
